{"cells":[{"cell_type":"markdown","metadata":{"id":"X6wimx0ZXlie"},"source":["##Preprocessing and Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"executionInfo":{"elapsed":445,"status":"error","timestamp":1728317936259,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"},"user_tz":-360},"id":"EiI_qv1dXRQd","outputId":"3d07b57d-cd8b-4b82-fa9a-07ef576569e3"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'satisfaction'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'satisfaction'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6cdb5a8607b4>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Encode the target variable 'satisfaction'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_df_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'satisfaction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'satisfaction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'satisfied'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dissatisfied'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Separate features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4089\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4090\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4091\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'satisfaction'"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.impute import SimpleImputer\n","\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","# Dropping the ID column (not useful for modeling) from both train and test\n","train_df_clean = train_df.drop(columns=['Id'])\n","test_df_clean = test_df.drop(columns=['Id'])\n","\n","# Impute missing values in the 'Arrival Delay in Minutes' with median\n","imputer = SimpleImputer(strategy='median')\n","train_df_clean['Arrival Delay in Minutes'] = imputer.fit_transform(train_df_clean[['Arrival Delay in Minutes']])\n","test_df_clean['Arrival Delay in Minutes'] = imputer.transform(test_df_clean[['Arrival Delay in Minutes']])\n","\n","# Encode categorical variables using LabelEncoder\n","categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n","\n","label_encoders = {}\n","for col in categorical_cols:\n","    le = LabelEncoder()\n","    train_df_clean[col] = le.fit_transform(train_df_clean[col])\n","    test_df_clean[col] = le.transform(test_df_clean[col])\n","    label_encoders[col] = le\n","\n","# Encode the target variable 'satisfaction'\n","train_df_clean['satisfaction'] = train_df_clean['satisfaction'].map({'satisfied': 1, 'dissatisfied': 0})\n","\n","# Separate features and target variable\n","X = train_df_clean.drop(columns=['satisfaction'])\n","y = train_df_clean['satisfaction']\n","\n","# Split the training data into training and validation sets (80-20 split)\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","X_train.head(), y_train.head()"]},{"cell_type":"markdown","metadata":{"id":"UJIW6n_5X6KO"},"source":["##Basic Random Forest Classifier Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23894,"status":"ok","timestamp":1728130121451,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"05993942951772728166"},"user_tz":-360},"id":"ZUGbqHCtX5Fc","outputId":"382251a7-1d33-48d6-aa72-cc83fbd4f5f8"},"outputs":[{"data":{"text/plain":["(0.9586380940159973,\n"," '              precision    recall  f1-score   support\\n\\n           0       0.94      0.97      0.95     10585\\n           1       0.97      0.95      0.96     12794\\n\\n    accuracy                           0.96     23379\\n   macro avg       0.96      0.96      0.96     23379\\nweighted avg       0.96      0.96      0.96     23379\\n')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Initialize the Random Forest Classifier\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Train the model\n","rf_model.fit(X_train, y_train)\n","\n","# Make predictions on the validation set\n","y_val_pred = rf_model.predict(X_val)\n","\n","# Evaluate the model performance\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","val_classification_report = classification_report(y_val, y_val_pred)\n","\n","val_accuracy, val_classification_report"]},{"cell_type":"markdown","metadata":{"id":"gzGxZ6NWyPlz"},"source":["First attempt with Basic RFC 5/10/24; score - 0.97 ~ 0.988033"]},{"cell_type":"markdown","metadata":{"id":"3Z2mm2TSYuN2"},"source":["##Prediction and Submission File Prep"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":979,"status":"ok","timestamp":1728137183867,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"05993942951772728166"},"user_tz":-360},"id":"Qnr1RFKvYzjO","outputId":"24e113d4-f30e-40c1-fd24-678ac1aee56c"},"outputs":[{"data":{"text/plain":["(       ID  satisfaction\n"," 0   46587  dissatisfied\n"," 1  124920     satisfied\n"," 2   18490     satisfied\n"," 3   78644  dissatisfied\n"," 4   92713     satisfied,\n"," '/content/drive/MyDrive/Code/DS_1101/Fly High With FDS/submission.csv')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Make predictions on the test dataset\n","test_predictions = rf_model.predict(test_df_clean)\n","\n","# Prepare the submission file\n","submission_df = pd.DataFrame({\n","    'ID': test_df['Id'],\n","    'satisfaction': test_predictions\n","})\n","\n","# Convert satisfaction values back to original labels ('satisfied', 'dissatisfied')\n","submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","# Save the submission file\n","submission_file_path = 'submission_RFC_reversed.csv'\n","submission_df.to_csv(submission_file_path, index=False)\n","\n","submission_df.head(), submission_file_path"]},{"cell_type":"markdown","metadata":{"id":"05fiQSNsdQ_y"},"source":["##Full Random Forest Implementation with Extended Preprocessing and Feature Engineering\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":927550,"status":"error","timestamp":1728313281714,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"},"user_tz":-360},"id":"dH6ROs0GabpG","outputId":"628f0e71-95da-4efe-d598-ccd9b02e9a0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7357f1e60343>\u001b[0m in \u001b[0;36m<cell line: 164>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;31m# Run Random Forest Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Black-Mesa-Survivors_11_RandomForest_Extra.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-7357f1e60343>\u001b[0m in \u001b[0;36mrandom_forest_model\u001b[0;34m(X, y, X_test, test_df, output_path)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Fit the RandomizedSearchCV model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Import required libraries\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import SelectFromModel\n","\n","# EDA and Data Visualization\n","def plot_feature_distribution(df, feature, title):\n","    plt.figure(figsize=(8, 6))\n","    sns.countplot(x=feature, data=df)\n","    plt.title(title)\n","    plt.xticks(rotation=45)\n","    plt.show()\n","\n","# Data Preprocessing\n","def preprocess_data(train_df, test_df):\n","    # Drop 'Id' column as it is not useful for modeling\n","    train_df_clean = train_df.drop(columns=['Id'])\n","    test_df_clean = test_df.drop(columns=['Id'])\n","\n","    # Handle missing values (Impute 'Arrival Delay in Minutes' with the median)\n","    imputer = SimpleImputer(strategy='median')\n","    train_df_clean['Arrival Delay in Minutes'] = imputer.fit_transform(train_df_clean[['Arrival Delay in Minutes']])\n","    test_df_clean['Arrival Delay in Minutes'] = imputer.transform(test_df_clean[['Arrival Delay in Minutes']])\n","\n","    # Feature Engineering: Add new features like total flights or flight categories if possible (domain-specific knowledge)\n","    # Example: Binning 'Flight Distance' into categories (short, medium, long flights)\n","    train_df_clean['Flight Distance Category'] = pd.cut(train_df_clean['Flight Distance'],\n","                                                       bins=[0, 1000, 3000, np.inf],\n","                                                       labels=['short', 'medium', 'long'])\n","    test_df_clean['Flight Distance Category'] = pd.cut(test_df_clean['Flight Distance'],\n","                                                      bins=[0, 1000, 3000, np.inf],\n","                                                      labels=['short', 'medium', 'long'])\n","\n","    # Encode categorical variables using LabelEncoder\n","    categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Flight Distance Category']\n","\n","    label_encoders = {}\n","    for col in categorical_cols:\n","        le = LabelEncoder()\n","        le.fit(pd.concat([train_df_clean[col], test_df_clean[col]], axis=0).unique())\n","        train_df_clean[col] = le.transform(train_df_clean[col])\n","        test_df_clean[col] = le.transform(test_df_clean[col])\n","        label_encoders[col] = le\n","\n","    # Encode the target variable 'satisfaction'\n","    train_df_clean['satisfaction'] = train_df_clean['satisfaction'].map({'satisfied': 1, 'dissatisfied': 0})\n","\n","    # Standardize features like 'Flight Distance', 'Departure Delay' and 'Arrival Delay'\n","    scaler = StandardScaler()\n","    numerical_cols = ['Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n","    train_df_clean[numerical_cols] = scaler.fit_transform(train_df_clean[numerical_cols])\n","    test_df_clean[numerical_cols] = scaler.transform(test_df_clean[numerical_cols])\n","\n","    return train_df_clean, test_df_clean\n","\n","# Random Forest with Hyperparameter Tuning and Cross-Validation\n","def random_forest_model(X, y, X_test, test_df, output_path):\n","    # Split the data into training and validation sets (80-20 split)\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","    # Define Random Forest model\n","    rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n","\n","    # Hyperparameter tuning grid\n","    param_distributions = {\n","        'n_estimators': [100, 200, 300, 400],\n","        'max_depth': [10, 20, 30, 40],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4],\n","        'bootstrap': [True, False],\n","        'max_features': ['auto', 'sqrt', 'log2']\n","    }\n","\n","    # Stratified K-Folds Cross-Validation\n","    skf = StratifiedKFold(n_splits=5)\n","\n","    # Randomized Search CV for hyperparameter tuning\n","    random_search = RandomizedSearchCV(\n","        estimator=rf_model,\n","        param_distributions=param_distributions,\n","        n_iter=50,\n","        cv=skf,\n","        scoring='accuracy',\n","        n_jobs=-1,\n","        verbose=2,\n","        random_state=42\n","    )\n","\n","    # Fit the RandomizedSearchCV model\n","    random_search.fit(X_train, y_train)\n","\n","    # Best hyperparameters\n","    print(\"Best hyperparameters found by RandomizedSearchCV:\")\n","    print(random_search.best_params_)\n","\n","    # Evaluate on the validation set\n","    y_val_pred = random_search.predict(X_val)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","    print(f\"Validation Accuracy: {val_accuracy}\")\n","    print(\"\\nClassification Report on Validation Set:\")\n","    print(classification_report(y_val, y_val_pred))\n","\n","    # Confusion Matrix for better insight\n","    cm = confusion_matrix(y_val, y_val_pred)\n","    sns.heatmap(cm, annot=True, fmt='d')\n","    plt.title('Confusion Matrix on Validation Set')\n","    plt.show()\n","\n","    # Feature Importance for potential feature selection\n","    importances = random_search.best_estimator_.feature_importances_\n","    indices = np.argsort(importances)[::-1]\n","    plt.figure(figsize=(12, 8))\n","    plt.title('Feature Importance')\n","    plt.bar(range(X_train.shape[1]), importances[indices], align='center')\n","    plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Select top features using SelectFromModel\n","    selector = SelectFromModel(random_search.best_estimator_, threshold=\"median\")\n","    X_train_selected = selector.fit_transform(X_train, y_train)\n","    X_test_selected = selector.transform(X_test)\n","\n","    # Refit model on selected features\n","    random_search.best_estimator_.fit(X_train_selected, y_train)\n","\n","    # Make predictions on the test set with selected features\n","    test_predictions = random_search.best_estimator_.predict(X_test_selected)\n","\n","    # Prepare the submission DataFrame\n","    submission_df = pd.DataFrame({\n","        'ID': test_df['Id'],  # Re-include 'Id' from the original test dataset\n","        'satisfaction': test_predictions\n","    })\n","\n","    # Convert 'satisfaction' back to original labels ('satisfied' or 'dissatisfied')\n","    submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","    # Save the submission file\n","    submission_df.to_csv(output_path, index=False)\n","    print(f\"Submission file saved to: {output_path}\")\n","\n","# Load data\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","# Preprocess the data\n","train_df_clean, test_df_clean = preprocess_data(train_df, test_df)\n","\n","# Separate features and target\n","X = train_df_clean.drop(columns=['satisfaction'])\n","y = train_df_clean['satisfaction']\n","\n","# Run Random Forest Model\n","output_path = 'Black-Mesa-Survivors_11_RandomForest_Extra.csv'\n","random_forest_model(X, y, test_df_clean, test_df, output_path)\n"]},{"cell_type":"markdown","metadata":{"id":"7v-qvRSmgCt5"},"source":["##XGBoost with GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246654,"status":"ok","timestamp":1728313721023,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"},"user_tz":-360},"id":"g0Z8bjr_gJvQ","outputId":"ecdff96e-cad1-40c3-ec28-b5b4852e84e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n","Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:08:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\n","    E.g. tree_method = \"hist\", device = \"cuda\"\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:08:40] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters found by RandomizedSearchCV:\n","{'subsample': 0.8, 'n_estimators': 300, 'max_depth': 30, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.7}\n","Validation Accuracy: 0.9619744214893708\n","\n","Classification Report on Validation Set:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.97      0.96     10585\n","           1       0.97      0.96      0.96     12794\n","\n","    accuracy                           0.96     23379\n","   macro avg       0.96      0.96      0.96     23379\n","weighted avg       0.96      0.96      0.96     23379\n","\n","Submission file saved to: Black-Mesa-Survivors_12_XGBoost_GPU.csv\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:08:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\n","    E.g. tree_method = \"hist\", device = \"cuda\"\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:08:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","Potential solutions:\n","- Use a data structure that matches the device ordinal in the booster.\n","- Set the device for booster before call to inplace_predict.\n","\n","This warning will only be shown once.\n","\n","  warnings.warn(smsg, UserWarning)\n"]}],"source":["# Install the GPU version of XGBoost if needed\n","!pip install xgboost\n","\n","# Import required libraries\n","import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","\n","# Data Preprocessing\n","def preprocess_data(train_df, test_df):\n","    # Drop 'Id' column as it is not useful for modeling\n","    train_df_clean = train_df.drop(columns=['Id'])\n","    test_df_clean = test_df.drop(columns=['Id'])\n","\n","    # Handle missing values (Impute 'Arrival Delay in Minutes' with the median)\n","    imputer = SimpleImputer(strategy='median')\n","    train_df_clean['Arrival Delay in Minutes'] = imputer.fit_transform(train_df_clean[['Arrival Delay in Minutes']])\n","    test_df_clean['Arrival Delay in Minutes'] = imputer.transform(test_df_clean[['Arrival Delay in Minutes']])\n","\n","    # Feature Engineering: Add new features like total flights or flight categories if possible (domain-specific knowledge)\n","    train_df_clean['Flight Distance Category'] = pd.cut(train_df_clean['Flight Distance'],\n","                                                       bins=[0, 1000, 3000, np.inf],\n","                                                       labels=['short', 'medium', 'long'])\n","    test_df_clean['Flight Distance Category'] = pd.cut(test_df_clean['Flight Distance'],\n","                                                      bins=[0, 1000, 3000, np.inf],\n","                                                      labels=['short', 'medium', 'long'])\n","\n","    # Encode categorical variables using LabelEncoder\n","    categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Flight Distance Category']\n","\n","    label_encoders = {}\n","    for col in categorical_cols:\n","        le = LabelEncoder()\n","        le.fit(pd.concat([train_df_clean[col], test_df_clean[col]], axis=0).unique())\n","        train_df_clean[col] = le.transform(train_df_clean[col])\n","        test_df_clean[col] = le.transform(test_df_clean[col])\n","        label_encoders[col] = le\n","\n","    # Encode the target variable 'satisfaction'\n","    train_df_clean['satisfaction'] = train_df_clean['satisfaction'].map({'satisfied': 1, 'dissatisfied': 0})\n","\n","    # Standardize features like 'Flight Distance', 'Departure Delay' and 'Arrival Delay'\n","    scaler = StandardScaler()\n","    numerical_cols = ['Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n","    train_df_clean[numerical_cols] = scaler.fit_transform(train_df_clean[numerical_cols])\n","    test_df_clean[numerical_cols] = scaler.transform(test_df_clean[numerical_cols])\n","\n","    return train_df_clean, test_df_clean\n","\n","# XGBoost with Hyperparameter Tuning\n","def xgboost_model(X, y, X_test, test_df, output_path):\n","    # Split the data into training and validation sets (80-20 split)\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","    # Define XGBoost model\n","    xgb_model = xgb.XGBClassifier(tree_method='gpu_hist', random_state=42, use_label_encoder=False)\n","\n","    # Hyperparameter tuning grid\n","    param_distributions = {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [10, 20, 30],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'subsample': [0.7, 0.8, 1.0],\n","        'colsample_bytree': [0.7, 0.8, 1.0],\n","        'gamma': [0, 1, 5]\n","    }\n","\n","    # Stratified K-Folds Cross-Validation\n","    skf = StratifiedKFold(n_splits=5)\n","\n","    # Randomized Search CV for hyperparameter tuning\n","    random_search = RandomizedSearchCV(\n","        estimator=xgb_model,\n","        param_distributions=param_distributions,\n","        n_iter=25,\n","        cv=skf,\n","        scoring='accuracy',\n","        n_jobs=-1,\n","        verbose=2,\n","        random_state=42\n","    )\n","\n","    # Fit the RandomizedSearchCV model\n","    random_search.fit(X_train, y_train)\n","\n","    # Best hyperparameters\n","    print(\"Best hyperparameters found by RandomizedSearchCV:\")\n","    print(random_search.best_params_)\n","\n","    # Evaluate on the validation set\n","    y_val_pred = random_search.predict(X_val)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","    print(f\"Validation Accuracy: {val_accuracy}\")\n","    print(\"\\nClassification Report on Validation Set:\")\n","    print(classification_report(y_val, y_val_pred))\n","\n","    # Make predictions on the test set\n","    test_predictions = random_search.predict(X_test)\n","\n","    # Prepare the submission DataFrame\n","    submission_df = pd.DataFrame({\n","        'ID': test_df['Id'],  # Re-include 'Id' from the original test dataset\n","        'satisfaction': test_predictions\n","    })\n","\n","    # Convert 'satisfaction' back to original labels ('satisfied' or 'dissatisfied')\n","    submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","    # Save the submission file\n","    submission_df.to_csv(output_path, index=False)\n","    print(f\"Submission file saved to: {output_path}\")\n","\n","# Load data\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","# Preprocess the data\n","train_df_clean, test_df_clean = preprocess_data(train_df, test_df)\n","\n","# Separate features and target\n","X = train_df_clean.drop(columns=['satisfaction'])\n","y = train_df_clean['satisfaction']\n","\n","# Run XGBoost Model with GPU acceleration\n","output_path = 'Black-Mesa-Survivors_12_XGBoost_GPU.csv'\n","xgboost_model(X, y, test_df_clean, test_df, output_path)"]},{"cell_type":"markdown","metadata":{"id":"4QFr_qwOiu4q"},"source":["##LightGBM with GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":920},"executionInfo":{"elapsed":29821,"status":"error","timestamp":1728314129191,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"},"user_tz":-360},"id":"Er5zXa_Tix7h","outputId":"1b24ce7c-0bba-4bc7-8e04-1fd012fdc464"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"]},{"ename":"ValueError","evalue":"\nAll the 100 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 955, in fit\n    self._Booster = train(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\", line 282, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 3641, in __init__\n    _safe_call(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 296, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: No OpenCL device found\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-aa05e9e727bb>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Run LightGBM with GPU acceleration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Black-Mesa-Survivors_12_LightGBM_GPU.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mlightgbm_gpu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-aa05e9e727bb>\u001b[0m in \u001b[0;36mlightgbm_gpu_model\u001b[0;34m(X, y, X_test, test_df, output_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Fit the RandomizedSearchCV model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m                     )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             )\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nAll the 100 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 955, in fit\n    self._Booster = train(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\", line 282, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 3641, in __init__\n    _safe_call(\n  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 296, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: No OpenCL device found\n"]}],"source":["# Install the GPU version of LightGBM if necessary\n","!pip install lightgbm --upgrade\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Data Preprocessing\n","def preprocess_data(train_df, test_df):\n","    # Drop 'Id' column as it is not useful for modeling\n","    train_df_clean = train_df.drop(columns=['Id'])\n","    test_df_clean = test_df.drop(columns=['Id'])\n","\n","    # Handle missing values (Impute 'Arrival Delay in Minutes' with the median)\n","    imputer = SimpleImputer(strategy='median')\n","    train_df_clean['Arrival Delay in Minutes'] = imputer.fit_transform(train_df_clean[['Arrival Delay in Minutes']])\n","    test_df_clean['Arrival Delay in Minutes'] = imputer.transform(test_df_clean[['Arrival Delay in Minutes']])\n","\n","    # Feature Engineering: Create flight distance categories\n","    train_df_clean['Flight Distance Category'] = pd.cut(train_df_clean['Flight Distance'],\n","                                                       bins=[0, 1000, 3000, np.inf],\n","                                                       labels=['short', 'medium', 'long'])\n","    test_df_clean['Flight Distance Category'] = pd.cut(test_df_clean['Flight Distance'],\n","                                                      bins=[0, 1000, 3000, np.inf],\n","                                                      labels=['short', 'medium', 'long'])\n","\n","    # Encode categorical variables using LabelEncoder\n","    categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Flight Distance Category']\n","\n","    label_encoders = {}\n","    for col in categorical_cols:\n","        le = LabelEncoder()\n","        le.fit(pd.concat([train_df_clean[col], test_df_clean[col]], axis=0).unique())\n","        train_df_clean[col] = le.transform(train_df_clean[col])\n","        test_df_clean[col] = le.transform(test_df_clean[col])\n","        label_encoders[col] = le\n","\n","    # Encode the target variable 'satisfaction'\n","    train_df_clean['satisfaction'] = train_df_clean['satisfaction'].map({'satisfied': 1, 'dissatisfied': 0})\n","\n","    # Standardize numeric features\n","    scaler = StandardScaler()\n","    numerical_cols = ['Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n","    train_df_clean[numerical_cols] = scaler.fit_transform(train_df_clean[numerical_cols])\n","    test_df_clean[numerical_cols] = scaler.transform(test_df_clean[numerical_cols])\n","\n","    return train_df_clean, test_df_clean\n","\n","# LightGBM with GPU support\n","def lightgbm_gpu_model(X, y, X_test, test_df, output_path):\n","    # Split the data into training and validation sets (80-20 split)\n","    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","    # Define the LightGBM model using GPU acceleration\n","    lgb_model = lgb.LGBMClassifier(boosting_type='gbdt',\n","                                   objective='binary',\n","                                   random_state=42,\n","                                   device='gpu')  # Specify GPU usage\n","\n","    # Hyperparameter tuning grid for LightGBM\n","    param_distributions = {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [10, 20, 30],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'subsample': [0.7, 0.8, 1.0],\n","        'colsample_bytree': [0.7, 0.8, 1.0],\n","        'num_leaves': [31, 64, 128]\n","    }\n","\n","    # Stratified K-Folds Cross-Validation\n","    skf = StratifiedKFold(n_splits=5)\n","\n","    # Randomized Search CV for hyperparameter tuning\n","    random_search = RandomizedSearchCV(\n","        estimator=lgb_model,\n","        param_distributions=param_distributions,\n","        n_iter=20,  # Adjusted for faster processing\n","        cv=skf,\n","        scoring='accuracy',\n","        n_jobs=-1,\n","        verbose=2,\n","        random_state=42\n","    )\n","\n","    # Fit the RandomizedSearchCV model\n","    random_search.fit(X_train, y_train)\n","\n","    # Best hyperparameters\n","    print(\"Best hyperparameters found by RandomizedSearchCV:\")\n","    print(random_search.best_params_)\n","\n","    # Evaluate on the validation set\n","    y_val_pred = random_search.predict(X_val)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","    print(f\"Validation Accuracy: {val_accuracy}\")\n","    print(\"\\nClassification Report on Validation Set:\")\n","    print(classification_report(y_val, y_val_pred))\n","\n","    # Make predictions on the test set\n","    test_predictions = random_search.predict(X_test)\n","\n","    # Prepare the submission DataFrame\n","    submission_df = pd.DataFrame({\n","        'ID': test_df['Id'],  # Re-include 'Id' from the original test dataset\n","        'satisfaction': test_predictions\n","    })\n","\n","    # Convert 'satisfaction' back to original labels ('satisfied' or 'dissatisfied')\n","    submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","    # Save the submission file\n","    submission_df.to_csv(output_path, index=False)\n","    print(f\"Submission file saved to: {output_path}\")\n","\n","# Load data\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","# Preprocess the data\n","train_df_clean, test_df_clean = preprocess_data(train_df, test_df)\n","\n","# Separate features and target\n","X = train_df_clean.drop(columns=['satisfaction'])\n","y = train_df_clean['satisfaction']\n","\n","# Run LightGBM with GPU acceleration\n","output_path = 'Black-Mesa-Survivors_12_LightGBM_GPU.csv'\n","lightgbm_gpu_model(X, y, test_df_clean, test_df, output_path)"]},{"cell_type":"markdown","metadata":{"id":"7Q2KJ-yykc-3"},"source":["##Full Implementation of cuML Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"67T0090Jk2KL","outputId":"677fc40f-c5e4-42b4-f4cb-44f83843b9f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Oct  7 15:28:20 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","Cloning into 'rapidsai-csp-utils'...\n","remote: Enumerating objects: 511, done.\u001b[K\n","remote: Counting objects: 100% (242/242), done.\u001b[K\n","remote: Compressing objects: 100% (151/151), done.\u001b[K\n","remote: Total 511 (delta 159), reused 124 (delta 91), pack-reused 269 (from 1)\u001b[K\n","Receiving objects: 100% (511/511), 163.95 KiB | 4.31 MiB/s, done.\n","Resolving deltas: 100% (261/261), done.\n","Collecting pynvml\n","  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n","Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 1.8 MB/s eta 0:00:00\n","Installing collected packages: pynvml\n","Successfully installed pynvml-11.5.3\n","Installing RAPIDS remaining 24.4.* libraries\n","Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n","Collecting cudf-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.4.1-cp310-cp310-manylinux_2_28_x86_64.whl (473.3 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 473.3/473.3 MB 2.7 MB/s eta 0:00:00\n","Collecting cuml-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cuml-cu12/cuml_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1200.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.7 MB/s eta 0:00:00\n","Collecting cugraph-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cugraph-cu12/cugraph_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1429.1 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 1.0 MB/s eta 0:00:00\n","Collecting cuspatial-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cuspatial-cu12/cuspatial_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137.8 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.8/137.8 MB 7.2 MB/s eta 0:00:00\n","Collecting cuproj-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cuproj-cu12/cuproj_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 920.9/920.9 kB 55.2 MB/s eta 0:00:00\n","Collecting cuxfilter-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cuxfilter-cu12/cuxfilter_cu12-24.4.1-py3-none-any.whl (83 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.5/83.5 kB 7.3 MB/s eta 0:00:00\n","Collecting cucim-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/cucim-cu12/cucim_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 78.6 MB/s eta 0:00:00\n","Collecting pylibraft-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/pylibraft-cu12/pylibraft_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (823.0 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.0/823.0 MB 1.4 MB/s eta 0:00:00\n","Collecting raft-dask-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/raft-dask-cu12/raft_dask_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170.1 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.1/170.1 MB 6.6 MB/s eta 0:00:00\n","Collecting nx-cugraph-cu12==24.4.*\n","  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-24.4.0-py3-none-any.whl (125 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.7/125.7 kB 10.4 MB/s eta 0:00:00\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.8)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (5.5.0)\n","Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.1)\n","Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (12.2.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (2024.6.1)\n","Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.60.0)\n","Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (1.26.4)\n","Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.2.10)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (24.1)\n","Collecting pandas<2.2.2dev0,>=2.0 (from cudf-cu12==24.4.*)\n","  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (3.20.3)\n","Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (0.3.0)\n","Collecting pyarrow<15.0.0a0,>=14.0.1 (from cudf-cu12==24.4.*)\n","  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (13.8.1)\n","Collecting rmm-cu12==24.4.* (from cudf-cu12==24.4.*)\n","  Downloading https://pypi.nvidia.com/rmm-cu12/rmm_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 59.0 MB/s eta 0:00:00\n","Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12==24.4.*) (4.12.2)\n","Collecting dask-cuda==24.4.* (from cuml-cu12==24.4.*)\n","  Downloading dask_cuda-24.4.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting dask-cudf-cu12==24.4.* (from cuml-cu12==24.4.*)\n","  Downloading https://pypi.nvidia.com/dask-cudf-cu12/dask_cudf_cu12-24.4.1-py3-none-any.whl (48 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 4.6 MB/s eta 0:00:00\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.4.2)\n","Collecting rapids-dask-dependency==24.4.* (from cuml-cu12==24.4.*)\n","  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-24.4.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cuml-cu12==24.4.*) (1.13.1)\n","Collecting treelite==4.1.2 (from cuml-cu12==24.4.*)\n","  Downloading treelite-4.1.2-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting pylibcugraph-cu12==24.4.* (from cugraph-cu12==24.4.*)\n","  Downloading https://pypi.nvidia.com/pylibcugraph-cu12/pylibcugraph_cu12-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1430.2 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 GB 986.5 kB/s eta 0:00:00\n","Collecting ucx-py-cu12==0.37.* (from cugraph-cu12==24.4.*)\n","  Downloading https://pypi.nvidia.com/ucx-py-cu12/ucx_py_cu12-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 68.7 MB/s eta 0:00:00\n","Requirement already satisfied: geopandas>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from cuspatial-cu12==24.4.*) (1.0.1)\n","Requirement already satisfied: bokeh>=3.1 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (3.4.3)\n","Collecting datashader>=0.15 (from cuxfilter-cu12==24.4.*)\n","  Downloading datashader-0.16.3-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: holoviews>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.19.1)\n","Collecting jupyter-server-proxy (from cuxfilter-cu12==24.4.*)\n","  Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from cuxfilter-cu12==24.4.*) (1.4.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (8.1.7)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from cucim-cu12==24.4.*) (0.4)\n","Collecting scikit-image<0.23.0a0,>=0.19.0 (from cucim-cu12==24.4.*)\n","  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12==24.4.*) (3.3)\n","Collecting pynvml<11.5,>=11.0.0 (from dask-cuda==24.4.*->cuml-cu12==24.4.*)\n","  Downloading pynvml-11.4.1-py3-none-any.whl.metadata (7.7 kB)\n","Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n","Collecting dask==2024.1.1 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n","  Downloading dask-2024.1.1-py3-none-any.whl.metadata (3.7 kB)\n","Collecting distributed==2024.1.1 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n","  Downloading distributed-2024.1.1-py3-none-any.whl.metadata (3.4 kB)\n","Collecting dask-expr==0.4.0 (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*)\n","  Downloading dask_expr-0.4.0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.1)\n","Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.0.2)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (8.4.0)\n","Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.1.4)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.8)\n","Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (5.9.5)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n","Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.3.3)\n","Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.13.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n","Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (1.3.0)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (10.4.0)\n","Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (2024.9.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12==24.4.*) (3.0.11)\n","Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.4.*) (0.8.2)\n","Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (3.1.0)\n","Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (1.0.0)\n","Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.1.1)\n","Collecting pyct (from datashader>=0.15->cuxfilter-cu12==24.4.*)\n","  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.32.3)\n","Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2024.9.0)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (0.10.0)\n","Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (3.6.1)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2.0.6)\n","Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.4.*) (3.0.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12==24.4.*) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.2)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.7)\n","Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.0.0)\n","Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (2.0.3)\n","Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (0.4.2)\n","Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (4.66.5)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (6.1.0)\n","Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2.35.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2024.9.20)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (3.10)\n","Requirement already satisfied: jupyter-server>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.24.0)\n","Collecting simpervisor>=1.0.0 (from jupyter-server-proxy->cuxfilter-cu12==24.4.*)\n","  Downloading simpervisor-1.0.0-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.1)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12==24.4.*) (2.18.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.1.5)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (3.7.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (23.1.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.2)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (6.5.4)\n","Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.10.4)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.21.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (24.0.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.18.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (0.1.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyogrio>=0.7.2->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (1.16.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.4.*) (0.5.1)\n","Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (1.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.4.*) (3.3.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.2.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.20.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.3.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.12.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.23.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (21.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.2.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.20.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.22)\n","Downloading dask_cuda-24.4.0-py3-none-any.whl (126 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.6/126.6 kB 4.3 MB/s eta 0:00:00\n","Downloading treelite-4.1.2-py3-none-manylinux2014_x86_64.whl (810 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 810.9/810.9 kB 20.3 MB/s eta 0:00:00\n","Downloading dask-2024.1.1-py3-none-any.whl (1.2 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 45.2 MB/s eta 0:00:00\n","Downloading dask_expr-0.4.0-py3-none-any.whl (161 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.7/161.7 kB 14.7 MB/s eta 0:00:00\n","Downloading distributed-2024.1.1-py3-none-any.whl (1.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 54.9 MB/s eta 0:00:00\n","Downloading datashader-0.16.3-py2.py3-none-any.whl (18.3 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 93.4 MB/s eta 0:00:00\n","Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 98.8 MB/s eta 0:00:00\n","Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.0/38.0 MB 17.3 MB/s eta 0:00:00\n","Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 77.2 MB/s eta 0:00:00\n","Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl (37 kB)\n","Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n","   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 3.9 MB/s eta 0:00:00\n","Downloading simpervisor-1.0.0-py3-none-any.whl (8.3 kB)\n","Downloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n","Installing collected packages: simpervisor, pynvml, pyct, pyarrow, ucx-py-cu12, treelite, scikit-image, rmm-cu12, pandas, dask, pylibraft-cu12, distributed, dask-expr, cuproj-cu12, cudf-cu12, cucim-cu12, rapids-dask-dependency, pylibcugraph-cu12, datashader, cuspatial-cu12, nx-cugraph-cu12, dask-cudf-cu12, dask-cuda, raft-dask-cu12, cuml-cu12, cugraph-cu12, jupyter-server-proxy, cuxfilter-cu12\n","  Attempting uninstall: pynvml\n","    Found existing installation: pynvml 11.5.3\n","    Uninstalling pynvml-11.5.3:\n","      Successfully uninstalled pynvml-11.5.3\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 16.1.0\n","    Uninstalling pyarrow-16.1.0:\n","      Successfully uninstalled pyarrow-16.1.0\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.24.0\n","    Uninstalling scikit-image-0.24.0:\n","      Successfully uninstalled scikit-image-0.24.0\n","  Attempting uninstall: rmm-cu12\n","    Found existing installation: rmm-cu12 24.6.0\n","    Uninstalling rmm-cu12-24.6.0:\n","      Successfully uninstalled rmm-cu12-24.6.0\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: dask\n","    Found existing installation: dask 2024.8.0\n","    Uninstalling dask-2024.8.0:\n","      Successfully uninstalled dask-2024.8.0\n","  Attempting uninstall: distributed\n","    Found existing installation: distributed 2024.8.0\n","    Uninstalling distributed-2024.8.0:\n","      Successfully uninstalled distributed-2024.8.0\n","  Attempting uninstall: cudf-cu12\n","    Found existing installation: cudf-cu12 24.6.1\n","    Uninstalling cudf-cu12-24.6.1:\n","      Successfully uninstalled cudf-cu12-24.6.1\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n","Successfully installed cucim-cu12-24.4.0 cudf-cu12-24.4.1 cugraph-cu12-24.4.0 cuml-cu12-24.4.0 cuproj-cu12-24.4.0 cuspatial-cu12-24.4.0 cuxfilter-cu12-24.4.1 dask-2024.1.1 dask-cuda-24.4.0 dask-cudf-cu12-24.4.1 dask-expr-0.4.0 datashader-0.16.3 distributed-2024.1.1 jupyter-server-proxy-4.4.0 nx-cugraph-cu12-24.4.0 pandas-2.2.1 pyarrow-14.0.2 pyct-0.5.0 pylibcugraph-cu12-24.4.0 pylibraft-cu12-24.4.0 pynvml-11.4.1 raft-dask-cu12-24.4.0 rapids-dask-dependency-24.4.1 rmm-cu12-24.4.0 scikit-image-0.22.0 simpervisor-1.0.0 treelite-4.1.2 ucx-py-cu12-0.37.0\n","\n","        ***********************************************************************\n","        The pip install of RAPIDS is complete.\n","        \n","        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n","        \n","        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n","\n","        Troubleshooting:\n","            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n","            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n","        ***********************************************************************\n","        \n"]}],"source":["!nvidia-smi\n","# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n","# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/pip-install.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3567,"status":"ok","timestamp":1728316067389,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"},"user_tz":-360},"id":"Qh7VhsL6krya","outputId":"8715ef89-36a9-448f-dd21-04857de108fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py:344: UserWarning: For reproducible results in Random Forest Classifier or for almost reproducible results in Random Forest Regressor, n_streams=1 is recommended. If n_streams is > 1, results may vary due to stream/thread timing differences, even when random_state is set\n","  return func(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.9558559329283942\n","\n","Classification Report on Validation Set:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95     10584\n","           1       0.97      0.95      0.96     12794\n","\n","    accuracy                           0.96     23378\n","   macro avg       0.95      0.96      0.96     23378\n","weighted avg       0.96      0.96      0.96     23378\n","\n","Submission file saved to: cuml_rf_submission.csv\n"]}],"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import cudf\n","import cuml\n","from cuml.ensemble import RandomForestClassifier as cuRF\n","from cuml.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.impute import SimpleImputer\n","\n","# Data Preprocessing Function\n","def preprocess_data(train_df, test_df):\n","    # Drop 'Id' column\n","    train_df_clean = train_df.drop(columns=['Id'])\n","    test_df_clean = test_df.drop(columns=['Id'])\n","\n","    # Handle missing values (Impute 'Arrival Delay in Minutes' with the median)\n","    imputer = SimpleImputer(strategy='median')\n","    train_df_clean['Arrival Delay in Minutes'] = imputer.fit_transform(train_df_clean[['Arrival Delay in Minutes']])\n","    test_df_clean['Arrival Delay in Minutes'] = imputer.transform(test_df_clean[['Arrival Delay in Minutes']])\n","\n","    # Feature Engineering: Create flight distance categories\n","    train_df_clean['Flight Distance Category'] = pd.cut(train_df_clean['Flight Distance'],\n","                                                       bins=[0, 1000, 3000, np.inf],\n","                                                       labels=['short', 'medium', 'long'])\n","    test_df_clean['Flight Distance Category'] = pd.cut(test_df_clean['Flight Distance'],\n","                                                      bins=[0, 1000, 3000, np.inf],\n","                                                      labels=['short', 'medium', 'long'])\n","\n","    # Encode categorical variables using LabelEncoder\n","    categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Flight Distance Category']\n","\n","    label_encoders = {}\n","    for col in categorical_cols:\n","        le = LabelEncoder()\n","        le.fit(pd.concat([train_df_clean[col], test_df_clean[col]], axis=0).unique())\n","        train_df_clean[col] = le.transform(train_df_clean[col])\n","        test_df_clean[col] = le.transform(test_df_clean[col])\n","        label_encoders[col] = le\n","\n","    # Encode the target variable 'satisfaction'\n","    train_df_clean['satisfaction'] = train_df_clean['satisfaction'].map({'satisfied': 1, 'dissatisfied': 0})\n","\n","    # Standardize numeric features\n","    scaler = StandardScaler()\n","    numerical_cols = ['Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n","    train_df_clean[numerical_cols] = scaler.fit_transform(train_df_clean[numerical_cols])\n","    test_df_clean[numerical_cols] = scaler.transform(test_df_clean[numerical_cols])\n","\n","    return train_df_clean, test_df_clean\n","\n","# Load the data\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","# Preprocess the data\n","train_df_clean, test_df_clean = preprocess_data(train_df, test_df)\n","\n","# Convert pandas dataframes to cuDF dataframes (GPU DataFrames)\n","X = cudf.DataFrame(train_df_clean.drop(columns=['satisfaction']).astype('float32'))\n","y = cudf.Series(train_df_clean['satisfaction'])\n","X_test = cudf.DataFrame(test_df_clean)\n","\n","# Split the data into training and validation sets (80-20 split) using cuML\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Train cuML Random Forest Classifier\n","cu_rf_model = cuRF(n_estimators=300, max_depth=20, random_state=42)\n","\n","# Fit the model\n","cu_rf_model.fit(X_train, y_train)\n","\n","# Make predictions on the validation set\n","y_val_pred = cu_rf_model.predict(X_val)\n","\n","# Convert cuDF results to numpy for accuracy scoring\n","y_val_pred = y_val_pred.to_numpy() # Corrected method\n","y_val = y_val.to_numpy()         # Corrected method\n","\n","# Evaluate performance on validation set\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy: {val_accuracy}\")\n","print(\"\\nClassification Report on Validation Set:\")\n","print(classification_report(y_val, y_val_pred))\n","\n","# Make predictions on the test set\n","test_predictions = cu_rf_model.predict(X_test)\n","\n","# Ensure test_predictions is numpy array\n","test_predictions = test_predictions.to_numpy() # convert to numpy\n","\n","# Prepare the submission DataFrame\n","submission_df = pd.DataFrame({\n","    'ID': test_df['Id'],  # Re-include 'Id' from the original test dataset\n","    'satisfaction': test_predictions\n","})\n","\n","# Convert 'satisfaction' back to original labels ('satisfied' or 'dissatisfied')\n","submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","# Save the submission file\n","output_path = 'cuml_rf_submission.csv'\n","submission_df.to_csv(output_path, index=False)\n","\n","print(f\"Submission file saved to: {output_path}\")"]},{"cell_type":"markdown","source":["##Tune-maxxing the Random Forest"],"metadata":{"id":"NGpQLDF8ubEP"}},{"cell_type":"code","source":["!pip install category_encoders\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from category_encoders import TargetEncoder\n","\n","# Define the preprocess_data function\n","def preprocess_data(train_df, test_df):\n","    # Separate features and target variable\n","    X = train_df.drop(['satisfaction', 'Id'], axis=1)\n","    y = train_df['satisfaction'].map({'satisfied': 1, 'neutral or dissatisfied': 0})\n","    X_test = test_df.drop('Id', axis=1)\n","\n","    # Identify numerical and categorical features\n","    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_features = X.select_dtypes(include=['object']).columns\n","\n","    # Create transformers for numerical and categorical features\n","    numerical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='median')),\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('target_encoder', TargetEncoder())  # Using TargetEncoder\n","    ])\n","\n","    # Combine transformers using ColumnTransformer\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numerical_transformer, numerical_features),\n","            ('cat', categorical_transformer, categorical_features)\n","        ])\n","\n","    # Handle missing values in the target variable (y) before fitting\n","    y = y.fillna(y.mode()[0]) # Fill missing values with the mode\n","\n","    # Fit and transform the training data\n","    X = preprocessor.fit_transform(X, y)\n","\n","    # Transform the test data\n","    X_test = preprocessor.transform(X_test)\n","\n","    return X, X_test, y\n","\n","# Load and preprocess\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","X, X_test, y = preprocess_data(train_df, test_df)  # Use the preprocessed data\n","\n","# Split data\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Hyperparameter Tuning with RandomizedSearchCV (More efficient than GridSearchCV)\n","param_dist = {\n","    'n_estimators': [100, 200, 300, 500],  # Explore a range\n","    'max_depth': [None, 10, 20, 30],         # Include None for full depth\n","    'min_samples_split': [2, 5, 10],        # Experiment with splitting criteria\n","    'min_samples_leaf': [1, 2, 4],          # Control leaf size\n","    'max_features': ['sqrt', 'log2', None], # Or a float between 0 and 1\n","    'bootstrap': [True, False],            # Bagging (with replacement) or Pasting\n","    'class_weight': [None, 'balanced', 'balanced_subsample'] # Adjust for class imbalance (if any)\n","}\n","\n","\n","\n","rf = RandomForestClassifier(random_state=42)\n","random_search = RandomizedSearchCV(\n","    rf, param_distributions=param_dist, n_iter=50,\n","    scoring='accuracy', cv=5, n_jobs=-1, verbose=2, random_state=42\n",")\n","\n","random_search.fit(X_train, y_train)\n","\n","\n","\n","# Evaluate and Print Best Parameters\n","print(\"Best Hyperparameters:\", random_search.best_params_)\n","best_rf = random_search.best_estimator_\n","\n","\n","y_pred = best_rf.predict(X_val)\n","accuracy = accuracy_score(y_val, y_pred)\n","print(f\"Validation Accuracy: {accuracy}\")\n","print(classification_report(y_val, y_pred))\n","\n","\n","# Train best model on full training data (if needed for final prediction on the test set). If not needed, skip and use random search for test predictions.\n","\n","best_rf.fit(X, y) # Fit on the full training set\n","\n","\n","# Make predictions on the test set\n","test_predictions = best_rf.predict(X_test)\n","\n","# Prepare submission\n","submission_df = pd.DataFrame({\n","    'ID': test_df['Id'],\n","    'satisfaction': test_predictions\n","})\n","submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","submission_df.to_csv('random_forest_tuned_submission.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmSWawliueUA","outputId":"7cbf5931-928b-490e-c989-9c520e6394aa","executionInfo":{"status":"ok","timestamp":1728328183377,"user_tz":-360,"elapsed":440503,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.4)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n","Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n","Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n","  _data = np.array(data, dtype=dtype, copy=copy,\n"]},{"output_type":"stream","name":"stdout","text":["Best Hyperparameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 10, 'class_weight': 'balanced_subsample', 'bootstrap': False}\n","Validation Accuracy: 1.0\n","              precision    recall  f1-score   support\n","\n","         1.0       1.00      1.00      1.00     23379\n","\n","    accuracy                           1.00     23379\n","   macro avg       1.00      1.00      1.00     23379\n","weighted avg       1.00      1.00      1.00     23379\n","\n"]}]},{"cell_type":"markdown","source":["##Stratified K-Fold Cross-Validation"],"metadata":{"id":"PwMx6aXXCGqR"}},{"cell_type":"code","source":["!pip install category_encoders\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from category_encoders import TargetEncoder\n","import warnings # Importing warnings library to ignore the warning in future.\n","warnings.filterwarnings(\"ignore\") # Code to ignore warnings in future.\n","\n","# Preprocessing function (same as before)\n","def preprocess_data(train_df, test_df):\n","    # Separate features and target variable\n","    X = train_df.drop(['satisfaction', 'Id'], axis=1)\n","    y = train_df['satisfaction'].map({'satisfied': 1, 'neutral or dissatisfied': 0})\n","    X_test = test_df.drop('Id', axis=1)\n","\n","    # Identify numerical and categorical features\n","    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_features = X.select_dtypes(include=['object']).columns\n","\n","    # Create transformers for numerical and categorical features\n","    numerical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='median')),\n","        ('scaler', StandardScaler())\n","    ])\n","\n","    categorical_transformer = Pipeline(steps=[\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('target_encoder', TargetEncoder())  # Using TargetEncoder\n","    ])\n","\n","    # Combine transformers using ColumnTransformer\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numerical_transformer, numerical_features),\n","            ('cat', categorical_transformer, categorical_features)\n","        ])\n","\n","    # Handle missing values in the target variable (y) before fitting\n","    y = y.fillna(y.mode()[0]) # Fill missing values with the mode\n","\n","    # Fit and transform the training data\n","    X = preprocessor.fit_transform(X, y)\n","\n","    # Transform the test data\n","    X_test = preprocessor.transform(X_test)\n","\n","    return X, X_test, y\n","\n","\n","# Load and preprocess\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","X, X_test, y = preprocess_data(train_df, test_df)\n","\n","# Stratified K-Fold Cross-Validation\n","n_splits = 5  # Number of folds (adjust as needed)\n","skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Store results for each fold\n","cv_scores = []\n","cv_classification_reports = []\n","test_predictions = [] # Initialize test predictions with size equal to no. of folds.\n","\n","for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","\n","    # Hyperparameter Tuning with RandomizedSearchCV inside each fold\n","    param_dist = {\n","    'n_estimators': [100, 200, 300, 500],  # Explore a range\n","    'max_depth': [None, 10, 20, 30],         # Include None for full depth\n","    'min_samples_split': [2, 5, 10],        # Experiment with splitting criteria\n","    'min_samples_leaf': [1, 2, 4],          # Control leaf size\n","    'max_features': ['sqrt', 'log2', None], # Or a float between 0 and 1\n","    'bootstrap': [True, False],            # Bagging (with replacement) or Pasting\n","    'class_weight': [None, 'balanced', 'balanced_subsample']} # Adjust for class imbalance (if any)\n","# Hyperparameter grid for Random Forest\n","\n","    rf = RandomForestClassifier(random_state=42)\n","    random_search = RandomizedSearchCV(\n","        rf, param_distributions=param_dist, n_iter=50,  # Increase n_iter for wider search\n","        scoring='accuracy', cv=5, n_jobs=-1, verbose=2, random_state=42\n","    )  # Inner cross-validation within each fold\n","\n","    random_search.fit(X_train, y_train)\n","\n","    best_rf = random_search.best_estimator_\n","\n","\n","    # Evaluate on validation fold\n","    y_pred = best_rf.predict(X_val)\n","    accuracy = accuracy_score(y_val, y_pred)\n","    classification_rep = classification_report(y_val, y_pred)\n","    cv_scores.append(accuracy)\n","    cv_classification_reports.append(classification_rep)\n","    test_predictions.append(best_rf.predict_proba(X_test))\n","\n","    print(f\"Fold {fold+1} Accuracy: {accuracy}\")\n","    print(f\"Fold {fold+1} Classification Report:\\n{classification_rep}\\n\")\n","\n","# Summarize CV performance\n","print(\"Average cross-validation accuracy:\", np.mean(cv_scores))\n","print(\"Standard deviation of cross-validation accuracy:\", np.std(cv_scores))\n","\n","\n","# Create a submission based on the average of multiple models.\n","averaged_predictions = np.mean(test_predictions, axis = 0)\n","submission_df = pd.DataFrame({\n","    'ID': test_df['Id'],\n","    'satisfaction': (averaged_predictions[:, 1] > 0.5).astype(int) # Access second column (index 1) for probability of class 1.\n","})\n","\n","submission_df['satisfaction'] = submission_df['satisfaction'].map({1: 'satisfied', 0: 'dissatisfied'})\n","\n","submission_df.to_csv(\"averaged_model_submission.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-YQtbvgO7QHR","outputId":"39c87031-ef97-4580-8912-a0979cc0a7e9","executionInfo":{"status":"error","timestamp":1728327456783,"user_tz":-360,"elapsed":78822,"user":{"displayName":"Athoye Mahmud (অথৈ)","userId":"12304427894401267860"}}},"execution_count":1,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Collecting category_encoders\n","  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n","Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n","Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: category_encoders\n","Successfully installed category_encoders-2.6.4\n","Fitting 5 folds for each of 50 candidates, totalling 250 fits\n","Fold 1 Accuracy: 1.0\n","Fold 1 Classification Report:\n","              precision    recall  f1-score   support\n","\n","         1.0       1.00      1.00      1.00     23379\n","\n","    accuracy                           1.00     23379\n","   macro avg       1.00      1.00      1.00     23379\n","weighted avg       1.00      1.00      1.00     23379\n","\n","\n","Fitting 5 folds for each of 50 candidates, totalling 250 fits\n","Fold 2 Accuracy: 1.0\n","Fold 2 Classification Report:\n","              precision    recall  f1-score   support\n","\n","         1.0       1.00      1.00      1.00     23379\n","\n","    accuracy                           1.00     23379\n","   macro avg       1.00      1.00      1.00     23379\n","weighted avg       1.00      1.00      1.00     23379\n","\n","\n","Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-57a6adcee182>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     )  # Inner cross-validation within each fold\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mbest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOOZ8NB+97KthajibvvQNXE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}